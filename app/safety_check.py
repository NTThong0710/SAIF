from detoxify import Detoxify
from transformers import (
    AutoProcessor, AutoModelForImageClassification,
    ViTForImageClassification, ViTFeatureExtractor,
    BlipProcessor, BlipForConditionalGeneration, pipeline
)
from PIL import Image
import torch

import re
from urllib.parse import urlparse, unquote

# Load model phÃ¡t hiá»‡n URL Ä‘á»™c háº¡i
classifier = pipeline("zero-shot-classification")

# Load models
detox_model = Detoxify('original')

nsfw_model_id = "Falconsai/nsfw_image_detection"
nsfw_processor = AutoProcessor.from_pretrained(nsfw_model_id)
nsfw_model = AutoModelForImageClassification.from_pretrained(nsfw_model_id)

violence_model_id = "jaranohaal/vit-base-violence-detection"
violence_model = ViTForImageClassification.from_pretrained(violence_model_id)
violence_processor = ViTFeatureExtractor.from_pretrained(violence_model_id)

# Load BLIP cho caption cá»§a áº£nh
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")


def is_prompt_safe(prompt: str):
    results = detox_model.predict(prompt)
    threshold = 0.5
    flagged = {label: score for label, score in results.items() if score > threshold}
    if flagged:
        return False, list(flagged.keys())
    return True, []

def generate_caption(image: Image.Image):
    inputs = blip_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        out = blip_model.generate(**inputs)
    caption = blip_processor.decode(out[0], skip_special_tokens=True)
    return caption

def check_nsfw_image(image: Image.Image) -> str:
    """Kiá»ƒm tra vÃ  tráº£ vá» káº¿t quáº£ NSFW cá»§a áº£nh"""
    # Xá»­ lÃ½ NSFW
    nsfw_inputs = nsfw_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        nsfw_outputs = nsfw_model(**nsfw_inputs)
        nsfw_probs = torch.nn.functional.softmax(nsfw_outputs.logits, dim=1)[0]
    
    nsfw_labels = list(nsfw_model.config.id2label.values())
    nsfw_pred = nsfw_probs.argmax().item()
    nsfw_label = nsfw_labels[nsfw_pred]
    nsfw_score = nsfw_probs[nsfw_pred].item() * 100
    
    # Táº¡o caption
    caption = generate_caption(image)
    
    # ÄÃ¡nh giÃ¡ káº¿t quáº£
    if nsfw_label.lower() in ["porn", "hentai", "sex", "nsfw"]:
        return f"""ğŸš¨ áº¢nh KHÃ”NG an toÃ n (NSFW):
- Loáº¡i: {nsfw_label}
- Äá»™ chÃ­nh xÃ¡c: {nsfw_score:.2f}%
- MÃ´ táº£: {caption}"""
    else:
        return f"""âœ… áº¢nh an toÃ n (NSFW):
- Loáº¡i: {nsfw_label}
- Äá»™ chÃ­nh xÃ¡c: {nsfw_score:.2f}%
- MÃ´ táº£: {caption}"""

def check_violence_image(image: Image.Image) -> str:
    """Kiá»ƒm tra vÃ  tráº£ vá» káº¿t quáº£ báº¡o lá»±c cá»§a áº£nh"""
    # Xá»­ lÃ½ báº¡o lá»±c
    violence_inputs = violence_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        violence_outputs = violence_model(**violence_inputs)
        violence_probs = torch.nn.functional.softmax(violence_outputs.logits, dim=1)[0]
    
    violence_labels = ["Non-Violent", "Violent"]
    violence_pred = violence_probs.argmax().item()
    violence_label = violence_labels[violence_pred]
    violence_score = violence_probs[violence_pred].item() * 100
    
    # Táº¡o caption
    caption = generate_caption(image)
    
    # ÄÃ¡nh giÃ¡ káº¿t quáº£
    is_violent = False
    if violence_label.lower() == "non-violent" and violence_score > 50:
        is_violent = True
    elif violence_label.lower() == "violent" and violence_score > 80:
        is_violent = True
    
    if is_violent:
        return f"""ğŸš¨ áº¢nh KHÃ”NG an toÃ n (Báº¡o lá»±c):
- Loáº¡i: {violence_label}
- Äá»™ chÃ­nh xÃ¡c: {violence_score:.2f}%
- MÃ´ táº£: {caption}"""
    else:
        return f"""âœ… áº¢nh an toÃ n (Báº¡o lá»±c):
- Loáº¡i: {violence_label}
- Äá»™ chÃ­nh xÃ¡c: {violence_score:.2f}%
- MÃ´ táº£: {caption}"""

# ===HÃ m check url===
def check_url(url: str):
    try:
        # Chuáº©n hÃ³a URL (decode cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t)
        decoded_url = unquote(url)
        parsed = urlparse(decoded_url)
        
        # Danh sÃ¡ch cáº£nh bÃ¡o
        warnings = []
        
        # 1. PhÃ¡t hiá»‡n IP thay vÃ¬ domain (http://203.0.113.45/...)
        if re.match(r'^https?://\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', decoded_url):
            warnings.append("ğŸš¨ Nguy hiá»ƒm: Truy cáº­p trá»±c tiáº¿p báº±ng IP (thÆ°á»ng dÃ¹ng cho táº¥n cÃ´ng)")
        
        # 2. PhÃ¡t hiá»‡n file thá»±c thi (gift-card.exe)
        if re.search(r'\.(exe|msi|bat|js|jar|apk|dmg)(\?|$)', parsed.path.lower()):
            warnings.append("ğŸš¨ Nguy hiá»ƒm: URL chá»©a file thá»±c thi cÃ³ thá»ƒ Ä‘á»™c háº¡i")
        
        # 3. PhÃ¡t hiá»‡n redirect Ä‘á»™c háº¡i (redirect?target=...)
        if 'redirect' in parsed.path.lower() or 'url=' in parsed.query.lower():
            warnings.append("âš ï¸ Cáº£nh bÃ¡o: URL chá»©a chá»©c nÄƒng redirect (cÃ³ thá»ƒ lá»«a Ä‘áº£o)")
        
        # 4. PhÃ¡t hiá»‡n kÃ½ tá»± Ä‘áº·c biá»‡t (/login%20%2F%00%3F%2F%2E%2E)
        if re.search(r'%[0-9a-f]{2}|[\x00-\x1f\x7f]', url):
            warnings.append("ğŸš¨ Nguy hiá»ƒm: URL chá»©a kÃ½ tá»± mÃ£ hÃ³a Ä‘Ã¡ng ngá» (cÃ³ thá»ƒ táº¥n cÃ´ng)")
        
        # 5. PhÃ¡t hiá»‡n domain giáº£ máº¡o (example.com@malicious-site.com)
        if '@' in parsed.netloc:
            warnings.append("ğŸš¨ Lá»«a Ä‘áº£o: URL chá»©a ká»¹ thuáº­t giáº£ máº¡o domain (user@fake-domain)")
        
        # 6. PhÃ¡t hiá»‡n domain giáº£ danh (secure.example-login.com)
        deceptive_domains = ['login', 'secure', 'account', 'verify', 'update']
        if any(keyword in parsed.netloc.lower() for keyword in deceptive_domains):
            warnings.append("âš ï¸ Cáº£nh bÃ¡o: Domain cÃ³ dáº¥u hiá»‡u giáº£ máº¡o dá»‹ch vá»¥ Ä‘Äƒng nháº­p")
        
        # 7. Kiá»ƒm tra giao thá»©c khÃ´ng mÃ£ hÃ³a
        if parsed.scheme == 'http':
            warnings.append("âš ï¸ Cáº£nh bÃ¡o: Káº¿t ná»‘i khÃ´ng mÃ£ hÃ³a (HTTP)")
        
        # Káº¿t há»£p vá»›i AI classifier
        ai_result = classifier(url, candidate_labels=["malicious", "safe"])
        ai_label = ai_result["labels"][0]
        ai_score = ai_result["scores"][0] * 100
        
        # Táº¡o bÃ¡o cÃ¡o
        report = {
            "url": url,
            "decoded_url": decoded_url,
            "domain": parsed.netloc,
            "path": parsed.path,
            "warnings": warnings,
            "ai_analysis": {
                "label": ai_label,
                "confidence": ai_score
            }
        }
        
        # Quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng
        if warnings or ai_label == "malicious":
            return format_report(report, is_safe=False)
        else:
            return format_report(report, is_safe=True)
            
    except Exception as e:
        return f"âš ï¸ Lá»—i khi phÃ¢n tÃ­ch URL: {str(e)}"

def format_report(report: dict, is_safe: bool):
    """Äá»‹nh dáº¡ng bÃ¡o cÃ¡o dá»… Ä‘á»c"""
    warning_text = "\n".join(f"- {w}" for w in report["warnings"]) if report["warnings"] else "- KhÃ´ng phÃ¡t hiá»‡n cáº£nh bÃ¡o"
    
    if not is_safe:
        return f"""ğŸš¨ URL KHÃ”NG AN TOÃ€N
ğŸ” PhÃ¢n tÃ­ch chi tiáº¿t:
â€¢ URL gá»‘c: {report['url']}
â€¢ Domain: {report['domain']}
â€¢ ÄÆ°á»ng dáº«n: {report['path']}

ğŸ“¢ Cáº¢NH BÃO:
{warning_text}

ğŸ¤– PhÃ¢n tÃ­ch AI:
- Káº¿t quáº£: {report['ai_analysis']['label']}
- Äá»™ tin cáº­y: {report['ai_analysis']['confidence']:.2f}%

ğŸ›¡ï¸ Khuyáº¿n nghá»‹: KHÃ”NG TRUY Cáº¬P!"""
    else:
        return f"""âœ… URL AN TOÃ€N
ğŸ” PhÃ¢n tÃ­ch chi tiáº¿t:
â€¢ URL gá»‘c: {report['url']}
â€¢ Domain: {report['domain']}

ğŸ¤– PhÃ¢n tÃ­ch AI:
- Káº¿t quáº£: {report['ai_analysis']['label']}
- Äá»™ tin cáº­y: {report['ai_analysis']['confidence']:.2f}%"""