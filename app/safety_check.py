from detoxify import Detoxify
from transformers import (
    AutoProcessor, AutoModelForImageClassification,
    ViTForImageClassification, ViTFeatureExtractor,
    BlipProcessor, BlipForConditionalGeneration, pipeline
)
from PIL import Image
import torch

# Load model ph√°t hi·ªán URL ƒë·ªôc h·∫°i
classifier = pipeline("zero-shot-classification")

# Load models
detox_model = Detoxify('original')

nsfw_model_id = "Falconsai/nsfw_image_detection"
nsfw_processor = AutoProcessor.from_pretrained(nsfw_model_id)
nsfw_model = AutoModelForImageClassification.from_pretrained(nsfw_model_id)

violence_model_id = "jaranohaal/vit-base-violence-detection"
violence_model = ViTForImageClassification.from_pretrained(violence_model_id)
violence_processor = ViTFeatureExtractor.from_pretrained(violence_model_id)

# Load BLIP cho caption c·ªßa ·∫£nh
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")


def is_prompt_safe(prompt: str):
    results = detox_model.predict(prompt)
    threshold = 0.5
    flagged = {label: score for label, score in results.items() if score > threshold}
    if flagged:
        return False, list(flagged.keys())
    return True, []

def generate_caption(image: Image.Image):
    inputs = blip_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        out = blip_model.generate(**inputs)
    caption = blip_processor.decode(out[0], skip_special_tokens=True)
    return caption

def check_nsfw_image(image: Image.Image) -> str:
    """Ki·ªÉm tra v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ NSFW c·ªßa ·∫£nh"""
    # X·ª≠ l√Ω NSFW
    nsfw_inputs = nsfw_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        nsfw_outputs = nsfw_model(**nsfw_inputs)
        nsfw_probs = torch.nn.functional.softmax(nsfw_outputs.logits, dim=1)[0]
    
    nsfw_labels = list(nsfw_model.config.id2label.values())
    nsfw_pred = nsfw_probs.argmax().item()
    nsfw_label = nsfw_labels[nsfw_pred]
    nsfw_score = nsfw_probs[nsfw_pred].item() * 100
    
    # T·∫°o caption
    caption = generate_caption(image)
    
    # ƒê√°nh gi√° k·∫øt qu·∫£
    if nsfw_label.lower() in ["porn", "hentai", "sex", "nsfw"]:
        return f"""üö® ·∫¢nh KH√îNG an to√†n (NSFW):
- Lo·∫°i: {nsfw_label}
- ƒê·ªô ch√≠nh x√°c: {nsfw_score:.2f}%
- M√¥ t·∫£: {caption}"""
    else:
        return f"""‚úÖ ·∫¢nh an to√†n (NSFW):
- Lo·∫°i: {nsfw_label}
- ƒê·ªô ch√≠nh x√°c: {nsfw_score:.2f}%
- M√¥ t·∫£: {caption}"""

def check_violence_image(image: Image.Image) -> str:
    """Ki·ªÉm tra v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ b·∫°o l·ª±c c·ªßa ·∫£nh"""
    # X·ª≠ l√Ω b·∫°o l·ª±c
    violence_inputs = violence_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        violence_outputs = violence_model(**violence_inputs)
        violence_probs = torch.nn.functional.softmax(violence_outputs.logits, dim=1)[0]
    
    violence_labels = ["Non-Violent", "Violent"]
    violence_pred = violence_probs.argmax().item()
    violence_label = violence_labels[violence_pred]
    violence_score = violence_probs[violence_pred].item() * 100
    
    # T·∫°o caption
    caption = generate_caption(image)
    
    # ƒê√°nh gi√° k·∫øt qu·∫£
    is_violent = False
    if violence_label.lower() == "non-violent" and violence_score > 50:
        is_violent = True
    elif violence_label.lower() == "violent" and violence_score > 80:
        is_violent = True
    
    if is_violent:
        return f"""üö® ·∫¢nh KH√îNG an to√†n (B·∫°o l·ª±c):
- Lo·∫°i: {violence_label}
- ƒê·ªô ch√≠nh x√°c: {violence_score:.2f}%
- M√¥ t·∫£: {caption}"""
    else:
        return f"""‚úÖ ·∫¢nh an to√†n (B·∫°o l·ª±c):
- Lo·∫°i: {violence_label}
- ƒê·ªô ch√≠nh x√°c: {violence_score:.2f}%
- M√¥ t·∫£: {caption}"""

# ===H√†m check url===
def check_url(url: str):
    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng URL c∆° b·∫£n
    if not url.startswith(('http://', 'https://')):
        return "‚ö†Ô∏è L·ªói: URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http:// ho·∫∑c https://"
    
    try:
        # Th√™m c√°c ƒë·∫∑c tr∆∞ng ph√°t hi·ªán URL ƒë√°ng ng·ªù
        suspicious_keywords = ['exe', 'download', 'free', 'gift', 'card']
        is_suspicious = any(keyword in url.lower() for keyword in suspicious_keywords)
        
        # √Åp d·ª•ng zero-shot classification
        result = classifier(url, candidate_labels=["malicious", "safe"])
        
        # L·∫•y k·∫øt qu·∫£ (ƒë√£ s·ª≠a c√°ch truy c·∫≠p)
        label = result["labels"][0]  # Nh√£n c√≥ ƒëi·ªÉm cao nh·∫•t
        score = result["scores"][0] * 100
        
        # K·∫øt h·ª£p c·∫£nh b√°o n·∫øu c√≥ t·ª´ kh√≥a ƒë√°ng ng·ªù
        warning = ""
        if is_suspicious:
            warning = "\n‚ö†Ô∏è C·∫£nh b√°o: URL ch·ª©a t·ª´ kh√≥a ƒë√°ng ng·ªù!"
        
        explanation = f"M√¥ h√¨nh ph√¢n lo·∫°i: {label} (ƒë·ªô tin c·∫≠y {score:.2f}%){warning}"
        
        if label.lower() == "malicious" or (score < 60 and is_suspicious):
            return f"""üö® URL KH√îNG an to√†n:
- K·∫øt qu·∫£: {label}
- {explanation}
- Ph√¢n t√≠ch: URL c√≥ ƒë·∫∑c ƒëi·ªÉm ƒë√°ng ng·ªù"""
        else:
            return f"""‚úÖ URL an to√†n:
- K·∫øt qu·∫£: {label}
- {explanation}"""
            
    except Exception as e:
        return f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra URL: {str(e)}"