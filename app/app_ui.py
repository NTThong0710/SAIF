import gradio as gr
from app.gen_ai import generate_response
from app.mlops_logger import log_prompt

# === Ki·ªÉm duy·ªát prompt ===
from detoxify import Detoxify

# === Ki·ªÉm duy·ªát ·∫£nh: NSFW + Violence ===
from transformers import AutoProcessor, AutoModelForImageClassification, ViTForImageClassification, ViTFeatureExtractor
from PIL import Image
import torch

# Load NSFW detector
nsfw_model_id = "Falconsai/nsfw_image_detection"
nsfw_processor = AutoProcessor.from_pretrained(nsfw_model_id)
nsfw_model = AutoModelForImageClassification.from_pretrained(nsfw_model_id)

# Load Violence detector
violence_model_id = "jaranohaal/vit-base-violence-detection"
violence_model = ViTForImageClassification.from_pretrained(violence_model_id)
violence_processor = ViTFeatureExtractor.from_pretrained(violence_model_id)

# ==== Ki·ªÉm duy·ªát h√¨nh ·∫£nh (image) ====
def check_image_safe(image: Image.Image):
    reasons = []
    
    # === NSFW Check ===
    nsfw_inputs = nsfw_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        nsfw_outputs = nsfw_model(**nsfw_inputs)
        nsfw_probs = torch.nn.functional.softmax(nsfw_outputs.logits, dim=1)[0]

    nsfw_labels = list(nsfw_model.config.id2label.values())
    nsfw_pred = nsfw_probs.argmax().item()
    nsfw_label = nsfw_labels[nsfw_pred]

    if nsfw_label.lower() in ["porn", "hentai", "sexy"] and nsfw_score > 55:
        reasons.append(f"·∫¢nh nh·∫°y c·∫£m ({nsfw_score:.2f}% - {nsfw_label})")

    # === Violence Check ===
    violence_inputs = violence_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        violence_outputs = violence_model(**violence_inputs)
        violence_probs = torch.nn.functional.softmax(violence_outputs.logits, dim=1)[0]

    violence_labels = list(violence_model.config.id2label.values())
    violence_confidences = {label: violence_probs[i].item() for i, label in enumerate(violence_labels)}
    violence_pred = violence_probs.argmax().item()
    violence_label = violence_labels[violence_pred]
    violence_score = violence_confidences[violence_label] * 100  # Convert to percentage

    # Ki·ªÉm tra Violence (s·ª≠ d·ª•ng label ch√≠nh x√°c t·ª´ model)
    if violence_label.lower() in ["violent", "violence", "label_1"] and violence_score > 50:
        reasons.append(f"·∫¢nh ch·ª©a b·∫°o l·ª±c ({violence_score:.2f}%)")

    # === T·ªïng k·∫øt ===
    if reasons:
        return f"üö® ·∫¢nh KH√îNG an to√†n:\n- " + "\n- ".join(reasons)
    else:
        return f"‚úÖ ·∫¢nh an to√†n\n- NSFW: {nsfw_label} ({nsfw_score:.2f}%)\n- Violence: {violence_label} ({violence_score:.2f}%)"

# === Ki·ªÉm duy·ªát Prompt ===
def is_prompt_safe(prompt: str):
    results = detox_model.predict(prompt)
    threshold = 0.5
    flagged = {label: score for label, score in results.items() if score > threshold}
    if flagged:
        return False, list(flagged.keys())
    return True, []

# === Giao di·ªán ===
with gr.Blocks(title="SAIFGuard - H·ªÜ TH·ªêNG KI·ªÇM DUY·ªÜT TH√îNG MINH", css="""
.yellow-btn {
    background-color: #FFD700 !important;
    color: black !important;
}
""") as demo:
    gr.Markdown("## üõ°Ô∏è SAIFGuard: H·ªÜ TH·ªêNG KI·ªÇM DUY·ªÜT TH√îNG MINH")
    
    with gr.Tab("üìù Ki·ªÉm duy·ªát Prompt"):
        with gr.Row():
            with gr.Column(scale=1):
                prompt_input = gr.Textbox(label="Nh·∫≠p Prompt", lines=2)
            with gr.Column(scale=1):
                prompt_status = gr.Textbox(label="Tr·∫°ng th√°i ki·ªÉm duy·ªát")
                prompt_output = gr.Textbox(label="K·∫øt qu·∫£ GenAI")
                prompt_button = gr.Button("Ki·ªÉm tra Prompt", elem_classes="yellow-btn")
        prompt_button.click(handle_prompt, inputs=prompt_input, outputs=[prompt_status, prompt_output])
    
    with gr.Tab("üñºÔ∏è Ki·ªÉm duy·ªát H√¨nh ·∫£nh"):
        with gr.Row():
            with gr.Column(scale=1):
                image_input = gr.Image(type="pil", label="T·∫£i ·∫£nh l√™n")
            with gr.Column(scale=1):
                image_output = gr.Textbox(label="Tr·∫°ng th√°i ki·ªÉm duy·ªát h√¨nh ·∫£nh")
                image_button = gr.Button("Ki·ªÉm tra H√¨nh ·∫£nh", elem_classes="yellow-btn")
        image_button.click(fn=check_image_safe, inputs=image_input, outputs=image_output)
